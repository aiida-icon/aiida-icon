{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"AiiDA-ICON - Documentation","text":""},{"location":"#about","title":"About","text":"<p>aiida-icon is a plugin to the AiiDA scientific workflow manager. It allows users of the ICON weather &amp; climate model to run experiment workflows which:</p> <ul> <li>remember all the inputs and configurations they were run with</li> <li>do not require careful file-structure setup on the target machine before running</li> <li>can be shared and adapted by colleagues in a group</li> <li>are reproducible</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<p>For users who do not want or need to learn how to leverage the full versatility of AiiDA, Sirocco provides a declarative way to describe Climate &amp; Weather workflows. This makes usage similar to other, field-specific workflow managers, without sacrificing the benefits of AiiDA. Many of the steps below will still apply if you want to use Sirocco, except that installing Sirocco will install aiida-icon as a dependency automatically.</p> <p>Here's how to get started with a minimal setup:</p>"},{"location":"#install","title":"Install","text":"<ol> <li> <p>Create and activate a python environment from which you will run your experiments. Conda, Virtualenvwrapper, venv etc are all fine ways to achieve that. In case you expect to be developing workflows and submission scripts, which you would like to be portable between machines, you should consider a version controlled project using a project manager like <code>uv</code>, <code>hatch</code> or <code>poetry</code>.</p> </li> <li> <p>Run <code>pip install aiida-icon</code> to install aiida-icon, AiiDA and all their dependencies. In the case of a project manager like <code>hatch</code>, use <code>hatch add aiida-icon</code> or your project manager's equivalent.</p> </li> </ol>"},{"location":"#setup-aiida","title":"Setup AiiDA","text":"<p>The following is a summary of the AiiDA Quick Install Guide and the AiiDA How To Run External Codes.</p> <ol> <li> <p>Install and run <code>rabbitmq 3.8.5</code>, either as a system service or as a docker image.</p> </li> <li> <p>Run <code>verdi presto</code> to create your AiiDA profile</p> </li> <li> <p>Run <code>verdi computer setup</code> to create a record of the target (HPC) machine you will use. This is not necessary if you are planning to run ICON on your local machine, as <code>verdi presto</code> has already created a record called <code>localhost</code> for your local machine. Here you will give that record a name, later referred to as <code>&lt;computername&gt;</code>.</p> </li> <li> <p>Allow AiiDA to connect to your target machine.</p> </li> <li>In general: run <code>verdi computer configure ssh &lt;computername&gt;</code> to allow AiiDA to connect to the target machine.</li> <li>For CSCS ALPS clusters: the recommended way is to<ol> <li>Run <code>pip install aiida-firecrest</code></li> <li>Set up firecrest connectivity to your target cluster using the CSCS Knowledge Base Article</li> <li>Run <code>verdi computer configure firecrest &lt;computername&gt;</code></li> </ol> </li> <li>Run <code>verdi computer test &lt;computername&gt;</code> to make sure a connection can be established.</li> </ol> <p>Warning</p> <p>You have to choose in step 3 already when you set \"Transport\" and \"Scheduler\", whether you will use \"ssh\" or \"firecrest\" in step 4. Pick \"core.ssh\" for transport and combine it with any of the available schedulers to connect with ssh OR pick \"firecrest\" for both (firecrest is used for both file transport as well as communication with the scheduler).</p> <ol> <li>Run <code>verdi code create icon</code> to set up an ICON executable on the computer you created in the previous step. Consult the AiiDA documentation for details.</li> </ol>"},{"location":"#get-productive","title":"Get Productive","text":"<p>Now you are ready to either run bare-bones ICON jobs (checkout the <code>examples/</code> directory for inspiration), or to develop AiiDA workflows incorporating ICON jobs.</p>"},{"location":"#run-from-all-local-namelist-files","title":"Run from all local namelist files","text":"<p>This is somewhat the best case for usability: AiiDA will remember all the parameters for you, as it stores the contents of the namelist files at the point of submitting the run.</p> <p>Of course, this requires that your master namelist specifies to look for model namelists in the work directory of the job. AiiDA will not upload files to elsewhere. For example:</p> <pre><code>&amp;master_nml\n  model_base_dir = models   ! or ./models, may not be an absolute path\n  ...\n/\n...\n&amp;master_model_nml\n  model_name = \"atm\"\n  model_namelist_filename = \"&lt;path&gt;/atm.namelist\"   ! referencing the relative \"model_base_dir\" as \"&lt;path&gt;\"\n  model_type = 1\n  ...\n/\n...\n</code></pre> <p>This will cause AiiDA-ICON to upload a copy of your local \"atm.namelist\" into the \"models\" sub directory of the work directory. This works for multiple model namelists too.</p> <pre><code>#!verdi run\nfrom aiida import orm\nfrom aiida_icon.calculations import IconCalculation\n\ncode = orm.load_code(\"myicon@myhpc\")\nbuilder = IconCalculation.get_builder()\nbuilder.code = code\nbuilder.master_namelist = orm.SinglefileData(\"/path/to/my/master.nml\")\nbuilder.models.atm = orm.SinglefileData(\"/path/to/my/atm.namelist\")\nbuilder.submit() # or .run() if you have not set up rabbitmq for your AiiDA profile\n</code></pre> <p>In this case, AiiDA keeps a snapshot of the contents of all the model namelist files for later reference - freeing you up to do with the files themselves as you wish.</p>"},{"location":"#run-an-icon-job-from-just-a-master-namelist-file","title":"Run an ICON job from just a master namelist file","text":"<p>The absolute minimum requirement is that you have the contents of your master namelist file available on the computer you submit from.</p> <p>Assume you have one that looks as follows:</p> <pre><code>&amp;master_nml\n  model_base_dir = /homes/myuser/models\n  ...\n/\n...\n&amp;master_model_nml\n  model_name = \"atm\"\n  model_namelist_filename = \"&lt;path&gt;/atm.namelist\"\n  model_type = 1\n  ...\n/\n...\n</code></pre> <p>This has to be in a file local to where you are running AiiDA. And you have the appropriate files in the appropriate places on your remote machine. You also have a code set up on your hpc system (named \"myhpc\" in your DB) and you named it \"myicon\".</p> <p>You can submit the job with the following script:</p> <pre><code>#!verdi run\nfrom aiida import orm\nfrom aiida_icon.calculations import IconCalculation\n\ncode = orm.load_code(\"myicon@myhpc\")\nbuilder = IconCalculation.get_builder()\nbuilder.code = code\nbuilder.master_namelist = orm.SinglefileData(\"/path/to/my/master.nml\")\nbuilder.submit() # or .run() if you have not set up rabbitmq for your AiiDA profile\n</code></pre> <p>Note that this will do two things with your master namelist file:</p> <ol> <li>It will keep the contents in your AiiDA database, forever connected to this run. You can change the file safely later and still inspect what it looked like when you ran this.</li> <li>It will upload a copy of it to the work directory of the job on your HPC machine (whether that be a remote one or your laptop), where it will also stay unchanged until cleaned up by you or by some file system policy.</li> </ol> <p>This means you keep a record of the exact parameters used, even if you change the file. It also means you can use that same set of parameters in future runs, even if the file is gone from your file system.</p>"},{"location":"#run-icon-job-with-provenance","title":"Run ICON job with provenance","text":"<p>Using the same ICON job as above: you might want to consider tracking the model namelist files your run is using inside the AiiDA graph. This makes it convenient to later inspect or retrieve them, without opening up the master namelist file first.</p> <pre><code>...\n\nbuilder.master_namelist = orm.SinglefileData(\"/path/to/my/master.nml\")\nbuilder.models.atm = orm.RemoteData(code.computer, remote_path=\"/homes/myuser/models/atm.namelist\")  # the 'remote_path' has to exactly match what the master namelist specifies\n</code></pre> <p>In this case, no additional files are moved, the <code>.models.atm</code> input is simply there to keep a record of what file path you used in the database. Looking back later, there is no way to even know if the file has been changed after this was run.</p>"},{"location":"#recipes-for-specific-use-cases","title":"Recipes for specific use cases","text":"<p>Find detailed instructions for specific known usecases here</p>"},{"location":"recipes/","title":"AiiDA-ICON Recipes","text":""},{"location":"recipes/#run-icon-via-a-wrapper-script","title":"Run ICON via a wrapper script","text":"<p>Warning</p> <p>None of the following will work if you opt out of running with MPI.</p> <p>On some HPC facilities it is common to run ICON using the following pattern</p> <pre><code>mpirun --&lt;options&gt; wrapper_script.sh &lt;/path/to/icon/binary&gt;\n</code></pre> <pre><code>srun --&lt;options&gt; wrapper_script.sh &lt;/path/to/icon/binary&gt;\n</code></pre> <p>etc, where the job of the <code>wrapper_script.sh</code> is to prepare environment variables and other things specific to the machine being used to obtain the best possible performance.</p> <p>Here is an example how to do this with AiiDA-ICON:</p> <pre><code>from aiida import orm\nfrom aiida_icon.calculations import IconCalculation\n\nbuilder = IconCalculation.get_builder()\nbuilder.wrapper_script = orm.SinglefileData(\"/path/to/my/wrapper_script.sh\")\n...\n</code></pre> <p>Currently you have to have a copy of this wrapper script locally, which AiiDA will upload for you. Alternatively you can create a <code>SinglefileData</code> instance from a string with the script contents.</p> <p>This is so that the AiiDA graph after the fact can record exactly how you ran ICON at the time.</p> <p>Note</p> <p>Adding the <code>.wrapper_script</code> input will automatically add a line to the <code>builder.metadata.options.prepend_text</code>. Take care not to delete it or you will get an error at run time that the wrapper script is not executable.</p> <p>Note</p> <p>Adding the <code>.wrapper_script</code> input will automatically add the wrapper script to <code>builder.metadata.options.mpirun_extra_params</code>. Take care to keep it as the last entry of that list (unless it takes additional arguments itself, those have to go afterwards).</p> <p>Since this affects also the prepend text as well as the parameters sent to the \"mpirun equivalent\" on your system, here is a continuation of the above example that shows how to manipulate those in the presence of a wrapper script.</p> <pre><code>builder.wrapper_script = orm.SinglefileData(\"/path/to/my/wrapper_script.sh\")\n...\noptions = builder.metadata.options\n# load a module before making the wrapper script executable\n# and print a message afterwards\noptions.prepend_text = f\"module load somemodule\\n{options.prepend_text}\\necho 'everything ready'\"\n# add an mpirun/srun/etc option\noptions.mpirun_extra_params.insert(-1, \"--myoption\")\n</code></pre> <p>Note</p> <p>AiiDA-ICON will rename the uploaded copy of your wrapper script to <code>run_icon.sh</code> for simplicity and</p> <p>universal readability.</p>"}]}